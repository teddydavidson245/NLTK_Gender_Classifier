# NLTK_Gender_Classifier

As the authors of chapter 6 of the nltk.org book approached the task, it was necessary to download and import the names corpus from NLTK. Two text documents containing male and female names were accessed from the names corpus, and those names were put in a joint list. The names were then shuffled randomly using the inbuilt Python function, random. Random shuffle helps prevent ‘hierarchical’ bias instances when selecting the feature sets after the two lists of names were appended to each other. The names are then stored as name + gender pairs. There are 7944 features: 794 for the validation set (10%), 794 for the testing set (10%), and 1588 for the training set (80%).
Improving on the feature generator gender_features () function provided, additional classifiers based on name length, first letter, and first and last name were built. The classifiers were trained on feature sets derived from the various feature generators instead of having all the features under one feature set to avoid overfitting. 
As one would have intuitively thought, the length of a name is not a good predictor of the gender a name belongs to. A Naïve Bayes classifier trained on name length has an accuracy of ~ 0.65 on the validation feature set. Additionally, checking the most informative features does not reveal any striking feature that could inform decision-making. For instance, words with three letters were the most informative features, but it indicated that that name is likely male at only 1.9% of the time. That is arguably a low likelihood. The classifier trained on the first names has a similarly poor performance. The accuracy of the classifier on the validation set is ~0.67. The most informative feature of that classifier is words that start with ‘W,’ which are likely male by a ratio of 4.7 to 1. That value is also too low to generalize. However, training a classifier based on the first and last name had a noticeably better performance, with an accuracy of ~0.80 on the validation set and ~0.79 on the testing set. The most informative features also had better scores. For instance, the most informative features based on the last letter ‘k’ with a score of 39.5 are likely male, e.g., Mark. The second most informative feature, the last letter ‘a’ with a score of 30.5, is likely female, e.g., Maria. With such scores, one could probably argue for those results. 
Interestingly, the classifier's precision, recall, and F1 scores were noticeably different for the two labels. As an illustration, I'd like you to see the following output: male precision: 0.7522123893805309, male recall: 0.5985959492929575746, male F-measure: 0.666666666666666666666. That difference can be attributed to the dataset imbalance (considerably more female names than male names), but more analysis would be needed to establish that. It might also be necessary to use a different dataset, but that is beyond the scope of the current analysis. As the report indicated earlier, the performance on the validation set is similar to that on the test set. Therefore, the results could be similar on the larger dataset and its source.
It is worthwhile to mention that similar results were observed when another NLTK classifier, Decision Tree, was used. The Decision Tree classifier had an accuracy of ~0.80, and similar behavior from earlier was observed in the precision, recall, and F1 scores. 
Additionally, as one would expect, the classifiers made more errors in predicting the male names than in the female names, as the error analysis revealed. 
Work referred to: 
6. Learning to Classify Text. (n.d.). Retrieved March 27, 2022, from https://www.nltk.org/book/ch06.html

